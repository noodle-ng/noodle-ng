#
# Noodle-NG - Crawler Configuration File
#
# In this file you provide configuration values for the crawler part of Noodle

[main]
# You need to specify the url to access your database
# %(here)s will be replaced with the parent directory of this file
#
# Specify the database for SQLAlchemy to use via turbogears.database
# %(here) may include a ':' character on Windows environments; this can
# invalidate the URI when specifying a SQLite db via path name
#
# sqlalchemy.url = postgres://username:password@hostname:port/databasename 
# sqlalchemy.url = mysql://username:password@hostname:port/databasename
sqlalchemy.url = sqlite:///%(here)s/somedb.db
sqlalchemy.echo = False

# Set the number of worker process the crawler shall use
processes = 10

# Set debug = true to enable debugging mode in the crawler 
# (more verbose output, no multiprocessing)
# This is not recommended in productional use
debug = false

# These credentials are (in this order) used to try to login to each share
# on each host defined in the locations below
#
# anonymous is a special case where no password needs to be provided
credentials = ["anonymous", ""], ["Gast", "123Dabei"]

# Now you need to define at least one location
[LAN]
# A location needs at least a list of IP adresses or IP networks which should be crawled
# Each entry can have one of the following formats:
#   '127.0.0.1'                # single ip
#   '192.168/16'               # CIDR network block
#   '10.0.0.1-10.0.0.19'       # inclusive range
# More info can be found here: http://code.google.com/p/python-iptools
range = 127.0.0.1, 192.168.0.0/24

# Furthermore, you can provide additional credentials that will only be used for crawling this location
credentials = ["user", "somesecretpassword"]

[some_other_location]
range = 192.168.1.2, 192.168.1.5
credentials = ["anotheruser", "anothersecretpassword"]
