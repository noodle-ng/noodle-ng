#
# Noodle-NG - Crawler Configuration File
#
# In this file you provide configuration values for the crawler part of Noodle

[main]
# You need to specify an url to access your database
# For using a sqlite database for testing or development, you can use %(here)s,
# which will be replaced with the parent directory of this file
# %(here) may include a ':' character on Windows environments; this can
# invalidate the URI when specifying a SQLite db via path name
# sqlalchemy.url = postgres://username:password@hostname:port/databasename 
# sqlalchemy.url = mysql://username:password@hostname:port/databasename
sqlalchemy.url = sqlite:///%(here)s/noodle.db
sqlalchemy.echo = False

# Set the number of worker process the crawler shall use
processes = 10

# Set debug = True to enable debugging mode in the crawler 
# (more verbose output, no multiprocessing)
# This is not recommended in productional use
debug = False

# Now you need to define at least one location
[LAN]
# You need to specify the type of share. Available types depend on the current state of
# development. At the moment there is smb and ftp available.
type = smb
# A location needs at least a list of IP adresses or IP networks which should be crawled
# Each entry can have one of the following formats:
#   '127.0.0.1'                # single ip
#   '192.168/16'               # CIDR network block
#   '10.0.0.1-10.0.0.19'       # inclusive range
# More info on the syntax can be found here: http://code.google.com/p/python-iptools
hosts = 127.0.0.1, 192.168.0.0/24

# Furthermore, you need to provide credentials that will be used for crawling this location
# anonymous is a special keyword for not using any credentials, so there is no password
credentials = user:somesecretpassword, anonymous

[some_other_location]
type = ftp
hosts = 192.168.1.2, 192.168.1.5
credentials = anotheruser:anothersecretpassword
