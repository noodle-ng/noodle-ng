#
# Noodle-NG - Crawler Configuration File
#
# In this file you provide configuration values for the crawler part of Noodle

[main]
# You need to specify an url to access your database
# For using a sqlite database for testing or development, you can use %(here)s,
# which will be replaced with the parent directory of this file
# %(here) may include a ':' character on Windows environments; this can
# invalidate the URI when specifying a SQLite db via path name
# sqlalchemy.url = postgres://username:password@hostname:port/databasename 
# sqlalchemy.url = mysql://username:password@hostname:port/databasename
sqlalchemy.url = sqlite:///%(here)s/noodle.db
sqlalchemy.echo = False

# Set the number of worker process the crawler shall use
# (Please note that setting this to 1 will still enable multiprocessing,
# but with just one worker, whereas setting this to 0 will disable multiprocessing)
processes = 0

# Configure verbosity level of logging output
(Possible values are: DEBUG, INFO, WARNING, ERROR, CRITICAL)
logging.level = DEBUG

# Set debug = True to enable debugging mode in the crawler 
# (more verbose output, no multiprocessing)
# Overwrites logging.level to DEBUG and processes to 0
# This is not recommended in productional use
debug = False

# Define how long unreachable hosts shall be kept in the database
keep_hosts = 4 weeks

# Now you need to define at least one location
[LAN]
# You need to specify the type of share. Available types depend on the current state of
# development. At the moment there is smb and ftp available.
type = smb
# A location needs at least a list of IP adresses or IP networks which should be crawled
# Each entry can have one of the following formats:
#   '127.0.0.1'                # single ip
#   '192.168/16'               # CIDR network block
#   '10.0.0.1-10.0.0.19'       # inclusive range
# More info on the syntax can be found here: http://code.google.com/p/python-iptools
hosts = 127.0.0.1, 192.168.0.0/24

# This entry controls if smb shares shall be tried to access without any credentials
anonymous = True

# Furthermore, you can provide additional credentials that will be used for crawling this location
credentials = user:secretpassword, peter:peterspass

[some_other_location]
type = ftp
hosts = 192.168.1.2, 192.168.1.5
credentials = anotheruser:anothersecretpassword
