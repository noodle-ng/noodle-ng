# in this file you can setup the crawler
# The %(here)s variable will be replaced with the parent directory of this file

[main]
# pick the form for your database
# %(here) may include a ':' character on Windows environments; this can
# invalidate the URI when specifying a SQLite db via path name
# sqlalchemy.url=postgres://username:password@hostname:port/databasename 
# sqlalchemy.url=mysql://username:password@hostname:port/databasename
sqlalchemy.url = sqlite:///%(here)s/somedb.db
sqlalchemy.echo = false

# Set the number of worker process to spawn and use
multiproccessing.processes=10

# Set debug=true to enable debugging mode in the crawler 
# (more verbose output, no multiprocessing)
debug=false

# the credentials are used to try to login into the share
# anonymous is a special case where no password needs to be provided
credentials=["anonymous", ""], ["Gast", "123Dabei"]


# now you can define locations which the crawler should use.
[LAN]
# a range defines a set of IP adresses which should be crawled
# it can have the following format:
#   '127.0.0.1'                # single ip
#   '192.168/16'               # CIDR network block
#   ('10.0.0.1', '10.0.0.19')  # inclusive range
# more info can be found here: http://code.google.com/p/python-iptools
range=127.0.0.1, 192.168.0.0/16
# furthermore you can add  more credentials
credentials=["user", "somesecretpassword"]

[some_other_location]
range=192.169.0.2
credentials=["user", "somesecretpassword"]